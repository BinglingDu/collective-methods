# Richard's experiments with Beautiful Soup
----
Tested in *python 2.7*

###### Bypass the paywall on The New Yorker
For educational purposes only
````python
from bs4 import BeautifulSoup
import urllib2

print('This script helps you to bypass the The New Yorker paywall and access the full text articles by snatching them from Google Cache')
print('Example URL: http://www.newyorker.com/magazine/2015/11/16/human-bondage')

url = raw_input('Enter a New Yorker URL: ')

if url == "":
    url = "http://www.newyorker.com/magazine/2015/11/16/human-bondage"

cacheurl = "http://webcache.googleusercontent.com/search?q=cache:" + url

print(cacheurl)

opener = urllib2.build_opener()
opener.addheaders = [('User-agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36')]
response = opener.open(cacheurl)


page = response.read()
soup = BeautifulSoup(page, "html.parser")

articlebody = soup.find("div", {"id": "articleBody"})
paragraphs = articlebody.find_all("p")
for tag in paragraphs:
    print(tag.get_text())
````

###### Downloading the course offerings of ITP into a CSV file
````python
from bs4 import BeautifulSoup
import urllib2
import csv

instructors = []
titles = []
times = []

page = urllib2.urlopen("https://itp.nyu.edu/registration/Schedule.php?year=2016&semester=Spring")
soup = BeautifulSoup(page, "html.parser")

results = soup.find_all("font", {"size" : "-1"}) # course title and instructor
minusthree = soup.find_all("font", {"size": "-3"}) # course times

for tag in results: 
    anchors = tag.find_all("a") # course title
    minustwo = tag.find_all("font", {"size": "-2"}) # course instructor
    for tag in minustwo:
        #print tag.text # course instructor
        instructors.append(tag.text)
    for tag in anchors:
        #print tag.text # course title
        titles.append(tag.text)
        

for tag in minusthree:
    btags = tag.find_all("b")
    for tag in btags:
        #print tag.text # course time
        times.append(tag.text)
 
rows = zip(titles,instructors,times)

with open('soup.csv', 'wb') as f:
    writer = csv.writer(f)
    for row in rows:
        writer.writerow(row)

````

###### Downloading the newest image from a Xiaomi Yi camera via Wi-Fi connection
To be used in conjunction with [this Xiaomi Yi library](https://github.com/deltaflyer4747/Xiaomi_Yi). Part of the code below is based on the aforementioned library. 

````python
import os, re, sys, time, socket, urllib2
from settings import camaddr
from settings import camport
from bs4 import BeautifulSoup

# by default, the script fetches the newest image from this directory and 
# process it for usage in processing. 

def takeXiaoyiPhoto():
    global url
    url = "http://192.168.42.1/DCIM/100MEDIA/"
    srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    srv.connect((camaddr, camport))
    srv.send('{"msg_id":257,"token":0}')

    data = srv.recv(512)
    print data
    if "rval" in data:
    	token = re.findall('"param": (.+) }',data)[0]	
    else:
	    data = srv.recv(512)
	    if "rval" in data:
		    token = re.findall('"param": (.+) }',data)[0]	


    tosend = '{"msg_id":769,"token":%s}' %token
    srv.send(tosend)
    srv.recv(512)
    time.sleep(1)

def parseUrl():
    html = urllib2.urlopen(url).read()

    soup = BeautifulSoup(html, "html.parser")

    last_link = soup.find_all('a', href=True)[-1]
    filename = re.sub('<[^>]*>', '', str(last_link))
    print "the file name is" + filename
    print url + filename + " downloading with urllib2"
    f = urllib2.urlopen(url + filename)
    data = f.read()
    with open(filename, "wb") as code:
        code.write(data)
    
takeXiaoyiPhoto()
parseUrl()
````
